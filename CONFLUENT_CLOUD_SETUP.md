# â˜ï¸ Confluent Cloud Kafka Setup Guide

## ğŸ“‹ Tá»•ng Quan

Dá»± Ã¡n sá»­ dá»¥ng **Confluent Cloud** (managed Kafka service) thay vÃ¬ self-hosted Kafka Ä‘á»ƒ:
- âœ… KhÃ´ng cáº§n quáº£n lÃ½ infrastructure (Zookeeper, Kafka brokers)
- âœ… Auto-scaling & high availability
- âœ… Built-in monitoring & alerting
- âœ… $400 FREE credits (Ä‘á»§ dÃ¹ng ~4 thÃ¡ng)
- âœ… SASL/SSL security máº·c Ä‘á»‹nh

---

## ğŸš€ HÆ°á»›ng Dáº«n Setup (5 phÃºt)

### **BÆ°á»›c 1: Táº¡o Confluent Cloud Account**

1. Truy cáº­p: https://confluent.cloud/signup
2. ÄÄƒng kÃ½ vá»›i email (hoáº·c Google/GitHub)
3. XÃ¡c nháº­n email
4. **Nháº­n $400 FREE credits** (khÃ´ng cáº§n credit card)

---

### **BÆ°á»›c 2: Táº¡o Kafka Cluster**

#### **2.1. Táº¡o Environment**
```
1. Login vÃ o Confluent Cloud Console
2. Click "Environments" â†’ "Add environment"
   - Name: football-streaming
   - Stream Governance Package: Essentials (FREE)
3. Click "Create"
```

#### **2.2. Táº¡o Kafka Cluster**
```
1. Trong environment "football-streaming"
2. Click "Add cluster"
3. Chá»n cluster type:
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ âœ… Basic (RECOMMENDED)                                   â”‚
   â”‚    - $0.00/hour base + $0.10/GB ingress                 â”‚
   â”‚    - Single zone, 99.5% uptime SLA                      â”‚
   â”‚    - Perfect for development & staging                   â”‚
   â”‚                                                          â”‚
   â”‚ âš ï¸  Standard                                             â”‚
   â”‚    - $1.50/hour base + data transfer                    â”‚
   â”‚    - Multi-zone, 99.95% uptime SLA                      â”‚
   â”‚                                                          â”‚
   â”‚ âŒ Dedicated                                             â”‚
   â”‚    - $1.00/CKU/hour (~$720/month)                       â”‚
   â”‚    - Production-grade                                    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
4. Chá»n Region:
   - AWS: us-east-1, us-west-2, ap-southeast-1 (Singapore)
   - GCP: us-central1, asia-southeast1
   - Azure: eastus, westeurope
   
   ğŸ’¡ Tip: Chá»n region gáº§n báº¡n nháº¥t Ä‘á»ƒ giáº£m latency

5. Cluster Name: football-kafka-cluster
6. Click "Launch cluster" (takes ~5 minutes)
```

#### **2.3. Táº¡o Kafka Topic**
```
1. VÃ o cluster vá»«a táº¡o
2. Click "Topics" â†’ "Create topic"
   - Topic name: live-match-events
   - Partitions: 3 (recommended)
   - Retention time: 7 days
   - Cleanup policy: delete
3. Click "Create"
```

---

### **BÆ°á»›c 3: Táº¡o API Key & Secret**

```
1. Trong cluster, click "API keys" (left sidebar)
2. Click "Add key"
3. Chá»n scope:
   - âœ… Global access (recommended for simplicity)
   - OR Specific topic (live-match-events)
4. Click "Generate API key"
5. **COPY & SAVE:**
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚ API Key:    XXXXXXXXXXXXXXXX               â”‚
   â”‚ API Secret: YYYYYYYYYYYYYYYYYYYYYYYYYYYY   â”‚
   â”‚                                            â”‚
   â”‚ âš ï¸  Secret chá»‰ hiá»‡n 1 láº§n! LÆ°u ngay!      â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
6. Download credentials hoáº·c copy manual
```

---

### **BÆ°á»›c 4: Láº¥y Bootstrap Servers**

```
1. Trong cluster, click "Cluster settings"
2. Copy "Bootstrap server":
   
   Example:
   pkc-xxxxx.ap-southeast-1.aws.confluent.cloud:9092
   
   Format:
   pkc-<cluster-id>.<region>.<provider>.confluent.cloud:9092
```

---

### **BÆ°á»›c 5: Cáº¥u HÃ¬nh Dá»± Ãn**

#### **5.1. Táº¡o file `.env`**

```bash
# Copy template
cp .env.example .env

# Edit vá»›i credentials cá»§a báº¡n
nano .env
```

#### **5.2. Äiá»n thÃ´ng tin:**

```bash
# ============================================================================
# CONFLUENT CLOUD KAFKA CONFIGURATION
# ============================================================================

# Bootstrap Servers (from Step 4)
KAFKA_BOOTSTRAP_SERVERS=pkc-xxxxx.ap-southeast-1.aws.confluent.cloud:9092

# API Credentials (from Step 3)
KAFKA_API_KEY=XXXXXXXXXXXXXXXX
KAFKA_API_SECRET=YYYYYYYYYYYYYYYYYYYYYYYYYYYY

# Topic Name
KAFKA_TOPIC=live-match-events

# ============================================================================
# FOOTBALL API
# ============================================================================
FOOTBALL_API_TOKEN=798a49800fe84474bc7858ca06434966
```

#### **5.3. Load environment variables:**

```bash
# Linux/Mac
export $(cat .env | xargs)

# OR use python-dotenv (recommended)
pip install python-dotenv
```

---

### **BÆ°á»›c 6: Test Connection**

#### **6.1. Install Dependencies**

```bash
pip install kafka-python confluent-kafka python-dotenv
```

#### **6.2. Test Producer**

```python
# test_confluent.py
from kafka import KafkaProducer
import json
import os
from dotenv import load_dotenv

load_dotenv()

producer = KafkaProducer(
    bootstrap_servers=os.getenv('KAFKA_BOOTSTRAP_SERVERS'),
    security_protocol='SASL_SSL',
    sasl_mechanism='PLAIN',
    sasl_plain_username=os.getenv('KAFKA_API_KEY'),
    sasl_plain_password=os.getenv('KAFKA_API_SECRET'),
    value_serializer=lambda v: json.dumps(v).encode('utf-8')
)

# Send test message
test_msg = {'test': 'Hello Confluent Cloud!', 'timestamp': '2025-01-15'}
future = producer.send('live-match-events', test_msg)
result = future.get(timeout=10)

print(f"âœ… Message sent! Partition: {result.partition}, Offset: {result.offset}")
producer.close()
```

```bash
python test_confluent.py
```

**Expected output:**
```
âœ… Message sent! Partition: 0, Offset: 123
```

#### **6.3. Verify in Confluent Cloud Console**

```
1. VÃ o Confluent Cloud Console
2. Navigate to: Cluster â†’ Topics â†’ live-match-events
3. Click "Messages" tab
4. Báº¡n sáº½ tháº¥y test message vá»«a gá»­i!
```

---

## ğŸ¯ Sá»­ Dá»¥ng Trong Dá»± Ãn

### **1. Start Producer**

```bash
# Load .env
export $(cat .env | xargs)

# Run producer
python src/streaming/live_events_producer.py
```

**Output:**
```
================================================================================
  LIVE MATCH EVENTS PRODUCER
  Kafka: Confluent Cloud (pkc-xxxxx.ap-southeast-1.aws.confluent.cloud:9092)
  Topic: live-match-events
  API: Football-Data.org
================================================================================
âœ… Kafka producer initialized (Confluent Cloud)
ğŸ”„ [14:35:22] Checking live matches...
ğŸ“¡ Fetched 3 live matches
ğŸ“¤ Sent 3/3 events to Kafka
```

### **2. Start Consumer**

```bash
# Terminal 2
export $(cat .env | xargs)

# Run consumer
python src/streaming/live_events_consumer.py
```

**Output:**
```
================================================================================
  LIVE EVENTS SPARK STREAMING CONSUMER
  Kafka: Confluent Cloud â†’ Topic: live-match-events
  Output: PostgreSQL streaming.live_events
  Micro-batch interval: 30 seconds
================================================================================
âœ… Spark session initialized with Kafka support
ğŸ“¡ Connecting to Confluent Cloud Kafka
âœ… Connected to Kafka topic: live-match-events
ğŸš€ Streaming started!
```

---

## ğŸ“Š Monitoring & Management

### **Confluent Cloud Console**

#### **1. Cluster Health**
```
Dashboard â†’ Cluster Overview
- Throughput (MB/s)
- Request rate (req/s)
- Active connections
- Storage used
```

#### **2. Topic Metrics**
```
Topics â†’ live-match-events â†’ Metrics
- Messages per second
- Bytes in/out
- Consumer lag
- Partition distribution
```

#### **3. Consumer Groups**
```
Consumers â†’ Consumer Groups
- Group ID: live-events-consumer
- Lag per partition
- Last offset committed
```

#### **4. Data Viewer**
```
Topics â†’ live-match-events â†’ Messages
- Browse messages in real-time
- Filter by partition/offset
- View message headers & payload
```

---

## ğŸ’° Cost Optimization

### **FREE Credits Usage ($400)**

**Basic Cluster Costs:**
```
Base:       $0.00/hour  (FREE tier)
Ingress:    $0.10/GB
Egress:     $0.09/GB
Storage:    $0.10/GB/month
```

**Example Calculation:**
```
Scenario: Live match streaming 24/7

Messages:
- 10 live matches/day
- 1 update/30s = 2 updates/min = 120 updates/hour
- 10 matches Ã— 120 = 1,200 messages/hour
- 1,200 Ã— 24 = 28,800 messages/day

Data:
- Average message size: ~2 KB
- Daily ingress: 28,800 Ã— 2 KB = 57.6 MB â‰ˆ 0.06 GB
- Monthly ingress: 0.06 Ã— 30 = 1.8 GB

Cost:
- Ingress: 1.8 GB Ã— $0.10 = $0.18/month
- Storage: 1.8 GB Ã— $0.10 = $0.18/month
- Total: ~$0.36/month

FREE $400 credits â†’ lasts ~1,111 months! ğŸ‰
```

**Tips to Stay in FREE Tier:**
- âœ… Use Basic cluster (not Standard/Dedicated)
- âœ… Set retention to 7 days (not infinite)
- âœ… Delete old topics khÃ´ng dÃ¹ng
- âœ… Monitor usage in Billing dashboard

---

## ğŸ”’ Security Best Practices

### **1. API Key Management**

```bash
# âœ… DO: Store in .env (gitignore)
KAFKA_API_KEY=xxx
KAFKA_API_SECRET=yyy

# âŒ DON'T: Hardcode in code
producer = KafkaProducer(
    sasl_plain_username='XXXXXXX',  # âŒ BAD!
    sasl_plain_password='YYYYYYY'   # âŒ BAD!
)
```

### **2. Rotate API Keys**

```
1. Confluent Cloud Console â†’ API Keys
2. Create new key
3. Update .env with new credentials
4. Test producer/consumer
5. Delete old key
```

### **3. Restrict Access**

```
- Create separate API keys for producer/consumer
- Use resource-level ACLs:
  * Producer: WRITE on topic
  * Consumer: READ on topic + consumer group
```

---

## ğŸ› ï¸ Troubleshooting

### **Error: Authentication failed**

```
Error: kafka.errors.AuthenticationFailedError

Solution:
1. Verify API Key/Secret in .env
2. Check key is not expired
3. Ensure key has correct permissions
```

### **Error: Topic not found**

```
Error: UnknownTopicOrPartitionError

Solution:
1. Create topic in Confluent Cloud Console
2. OR enable auto.create.topics.enable (not recommended for production)
```

### **Error: Connection timeout**

```
Error: KafkaTimeoutError: Failed to update metadata

Solution:
1. Check bootstrap servers URL (typo?)
2. Verify network connectivity:
   telnet pkc-xxxxx.region.provider.confluent.cloud 9092
3. Check firewall rules (port 9092 must be open)
```

### **High Latency**

```
Symptoms: Messages take >5 seconds to appear

Solution:
1. Use geographically closer region
2. Increase partitions (3 â†’ 6)
3. Check producer batch settings:
   linger_ms=10  # Wait max 10ms before sending
   batch_size=32768  # 32KB batches
```

---

## ğŸ“š Confluent CLI (Optional)

### **Install CLI**

```bash
# Mac
brew install confluent-cli

# Linux
curl -sL --http1.1 https://cnfl.io/cli | sh -s -- latest

# Verify
confluent version
```

### **Login**

```bash
confluent login --save
```

### **Useful Commands**

```bash
# List environments
confluent environment list

# Use environment
confluent environment use env-xxxxx

# List clusters
confluent kafka cluster list

# Use cluster
confluent kafka cluster use lkc-xxxxx

# List topics
confluent kafka topic list

# Describe topic
confluent kafka topic describe live-match-events

# Produce message
echo '{"test":"message"}' | confluent kafka topic produce live-match-events

# Consume messages
confluent kafka topic consume live-match-events --from-beginning

# Create API key
confluent api-key create --resource lkc-xxxxx
```

---

## ğŸ”— Useful Links

- **Confluent Cloud Console**: https://confluent.cloud/
- **Documentation**: https://docs.confluent.io/cloud/current/
- **Pricing Calculator**: https://www.confluent.io/confluent-cloud/pricing/
- **Free Trial**: https://www.confluent.io/confluent-cloud/tryfree/
- **Support**: https://support.confluent.io/

---

## âœ… Checklist

- [ ] Created Confluent Cloud account ($400 FREE credits)
- [ ] Created environment & Basic cluster
- [ ] Created topic: live-match-events (3 partitions)
- [ ] Generated API Key & Secret
- [ ] Copied Bootstrap Servers URL
- [ ] Updated `.env` with credentials
- [ ] Tested producer connection
- [ ] Tested consumer connection
- [ ] Verified messages in Console
- [ ] Set up monitoring alerts (optional)

---

## ğŸ“ Next Steps

1. âœ… Setup Confluent Cloud (this guide)
2. â­ï¸ Run producer: `python src/streaming/live_events_producer.py`
3. â­ï¸ Run consumer: `python src/streaming/live_events_consumer.py`
4. â­ï¸ Query live data: `SELECT * FROM streaming.vw_current_live_matches;`
5. â­ï¸ Build dashboard with Grafana/Metabase

---

**ğŸ‰ BÃ¢y giá» báº¡n Ä‘Ã£ cÃ³ Kafka cluster production-ready trÃªn cloud!**

**Cost: ~$0.36/month** (vá»›i $400 FREE credits â†’ 1000+ months) ğŸš€
