# âš½ Football Big Data Analytics Platform

[![Python](https://img.shields.io/badge/Python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![Spark](https://img.shields.io/badge/Spark-4.0.0-orange.svg)](https://spark.apache.org/)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-14+-blue.svg)](https://www.postgresql.org/)
[![License](https://img.shields.io/badge/License-Educational-green.svg)](LICENSE)

A **production-ready Big Data analytics platform** for football data with **batch ETL** (5.6M+ records) and **real-time streaming** (Kafka + Spark) pipelines. Features **Medallion architecture** (Bronze â†’ Silver â†’ Gold), **Apache NiFi** visual data flows, and **Apache Superset** dashboards.

---

## ğŸ“Œ Table of Contents

- [âœ¨ Key Features](#-key-features)
- [ğŸ“¦ Dataset](#-dataset)
- [ğŸ—ï¸ Architecture](#ï¸-architecture)
- [ğŸš€ Quick Start](#-quick-start)
- [ğŸ“Š Data Overview](#-data-overview)
- [ğŸ’» Sample Queries](#-sample-queries)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ“š Documentation](#-documentation)
- [ğŸ”§ Configuration](#-configuration)
- [ğŸ“ˆ Performance](#-performance)
- [ğŸ¯ Use Cases](#-use-cases)

---

## âœ¨ Key Features

### **ğŸ¥‰ Batch Processing (ETL Pipeline)**
- âœ… **Medallion Architecture**: Bronze (raw) â†’ Silver (cleaned) â†’ Gold (analytics)
- âœ… **5.6M+ Records**: Player profiles, performances, transfers, injuries, market values
- âœ… **11 Data Sources**: CSV files from Transfermarkt dataset
- âœ… **Parquet Storage**: Optimized columnar format for analytics
- âœ… **PostgreSQL Integration**: 15 tables, 5.9M records, 50+ indexes
- âœ… **Data Quality Validation**: 8 automated quality checks
- âœ… **Views & Materialized Views**: 5 views + 3 materialized views
- âœ… **One Command Execution**: `python run_pipeline.py` (~2.5 minutes)

### **ğŸ”´ Real-Time Streaming Pipeline**
- âœ… **Apache NiFi Producer**: Visual data flow from Football-Data.org API
- âœ… **Confluent Cloud Kafka**: Managed message queue (SASL/SSL)
- âœ… **Spark Structured Streaming**: 30-second microbatch processing
- âœ… **Live Match Updates**: Real-time scores, events, statistics
- âœ… **UPSERT Logic**: Incremental updates to PostgreSQL
- âœ… **HYBRID Schema**: JSONB for flexible event storage

#### **Apache NiFi Data Flows**

<table>
  <tr>
    <td align="center">
      <img src="nifi/live-matches.png" width="350px" alt="Live Matches Flow"/><br />
      <b>Live Matches Flow</b><br />
      Fetch match events every 10s â†’ Kafka
    </td>
    <td align="center">
      <img src="nifi/live-competitions.png" width="350px" alt="Live Competitions Flow"/><br />
      <b>Live Competitions Flow</b><br />
      Fetch competitions every 60s â†’ Kafka
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="nifi/live-leaderboards.png" width="350px" alt="Live Leaderboards Flow"/><br />
      <b>Live Leaderboards Flow</b><br />
      Fetch leaderboards every 60s â†’ Kafka
    </td>
    <td align="center">
      <img src="nifi/competitions-to-leaderboards.png" width="350px" alt="Competitions to Leaderboards"/><br />
      <b>Competitions â†’ Leaderboards</b><br />
      Parse competition IDs for leaderboard fetching
    </td>
  </tr>
</table>

### **ğŸ“Š Analytics & Visualization**
- âœ… **5 Gold Analytics Tables**: 403K records with comprehensive metrics
- âœ… **Player 360Â° Profiles**: Overall scores, form metrics, market trends
- âœ… **Injury Risk Analysis**: Health scores and availability predictions
- âœ… **Transfer Intelligence**: Market value trends and opportunities
- âœ… **Apache Superset Dashboards**: Interactive visualizations

#### **Dashboard Screenshots**

<table>
  <tr>
    <td align="center">
      <img src="dashboards/Player-Scouting.jpg" width="400px" alt="Player Scouting Dashboard"/><br />
      <b>Player Scouting</b><br />
      Top performers analysis by position and metrics
    </td>
    <td align="center">
      <img src="dashboards/Performance-Analytics.jpg" width="400px" alt="Performance Analytics Dashboard"/><br />
      <b>Performance Analytics</b><br />
      Player form metrics and statistical trends
    </td>
  </tr>
  <tr>
    <td align="center">
      <img src="dashboards/Transfer-Market-Intelligence.jpg" width="400px" alt="Transfer Market Dashboard"/><br />
      <b>Transfer Market Intelligence</b><br />
      Market value trends and transfer opportunities
    </td>
    <td align="center">
      <img src="dashboards/Injury-Risk-Management.jpg" width="400px" alt="Injury Risk Dashboard"/><br />
      <b>Injury Risk Management</b><br />
      Health scores and injury history analysis
    </td>
  </tr>
  <tr>
    <td align="center" colspan="2">
      <img src="dashboards/Football-Leaderboards.jpg" width="400px" alt="Football Leaderboards Dashboard"/><br />
      <b>Football Leaderboards</b><br />
      Competition standings and team rankings
    </td>
  </tr>
</table>

### **ğŸ”’ Security & Best Practices**
- âœ… **Environment Variables**: All credentials in `.env` (gitignored)
- âœ… **No Hardcoded Secrets**: 100% secure configuration
- âœ… **Template File**: `.env.sample` for easy setup
- âœ… **Production Ready**: Tested with real data

---

## ğŸ“¦ Dataset

ğŸ”— **Download Dataset**: [Google Drive - Football Analytics Dataset](https://drive.google.com/drive/folders/1ha8wBkS4s1vzXKt94CD_roIAgKE3A3yG?usp=sharing)

- **11 CSV files** from Transfermarkt
- **5.6M+ records** of football data
- **Player profiles, performances, transfers, injuries, market values**
- Place downloaded files in `football-datasets/datalake/transfermarkt/` directory

---

## ğŸ—ï¸ Architecture

### **System Overview**

```mermaid
graph TB
    A[CSV Data Sources<br/>11 files, 5.6M records] --> B[Bronze Layer<br/>Parquet - Raw Ingestion]
    B --> C[Silver Layer<br/>Parquet - Data Cleaning]
    C --> D[Gold Layer<br/>Parquet - Analytics]
    D --> E[PostgreSQL Warehouse<br/>5.9M records, 15 tables]
    E --> F[Views & Materialized Views<br/>8 total]
    
    G[Football API<br/>football-data.org] --> H[Apache NiFi<br/>Visual Producer]
    H --> I[Confluent Kafka<br/>Managed Queue]
    I --> J[Spark Streaming<br/>Consumer]
    J --> K[PostgreSQL<br/>Streaming Schema]
    K --> F
    
    F --> L[Apache Superset<br/>Dashboards]
    
    style B fill:#cd7f32
    style C fill:#c0c0c0
    style D fill:#ffd700
    style I fill:#ff6b6b
    style L fill:#4ecdc4
```

### **Data Flow**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        BATCH PIPELINE                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CSV (11 files)  â†’  Bronze  â†’  Silver  â†’  Gold  â†’  PostgreSQL   â”‚
â”‚    5.6M records      Parquet    Cleaned    Analytics   15 tablesâ”‚
â”‚                                                                  â”‚
â”‚  Runtime: ~2.5 minutes | Storage: 800 MB Parquet + 935 MB DB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     STREAMING PIPELINE                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  API â†’ NiFi â†’ Kafka â†’ Spark Streaming â†’ PostgreSQL â†’ Superset   â”‚
â”‚                                                                  â”‚
â”‚  Latency: 15-60s microbatch | Format: JSONB | Auth: SASL/SSL   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Streaming Data Flow Details**

**1. Match Events Streaming** (Real-time match updates)
```
Football-Data.org API
    â†“ (Every 10 seconds)
Apache NiFi InvokeHTTP Processor
    â†“ (Parse JSON, Route by status)
Kafka Topic: live-match-events
    â†“ (Consume with 15s trigger)
Spark Structured Streaming
    â†“ (UPSERT logic with JSONB)
PostgreSQL: streaming.football_matches
    â†“ (Query via views)
Apache Superset Dashboards
```

**2. Competitions & Leaderboards Streaming** (Tournament standings)
```
Football-Data.org API
    â†“ (Every 60 seconds)
Apache NiFi InvokeHTTP Processor
    â†“ (Parse competitions, extract IDs)
Kafka Topics: football-competitions, football-leaderboards
    â†“ (Consume with 60s trigger)
Spark Structured Streaming
    â†“ (UPSERT logic with separate tables)
PostgreSQL: streaming.competitions, streaming.leaderboards
    â†“ (Join with matches for complete view)
Apache Superset Dashboards
```

---

## ğŸš€ Quick Start

### **Prerequisites**
- Python 3.11+
- PostgreSQL 14+
- Apache Spark 4.0 (auto-installed via PySpark)
- Apache NiFi 1.25.0 (optional, for streaming)
- Apache Superset 3.0.0 (optional, for dashboards)

### **1. Environment Setup**

```bash
# Clone repository
git clone https://github.com/Flourish04/football_bigdata_analysis.git
cd football_project

# Create environment file
cp .env.sample .env

# Edit .env and add your credentials
nano .env
```

**Required Environment Variables:**
```bash
# PostgreSQL Configuration
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
POSTGRES_DB=football_analytics
POSTGRES_USER=postgres
POSTGRES_PASSWORD=your_postgres_password

# Kafka Configuration (for streaming)
KAFKA_BOOTSTRAP_SERVERS=pkc-xxx.aws.confluent.cloud:9092
KAFKA_API_KEY=your_kafka_api_key
KAFKA_API_SECRET=your_kafka_api_secret
KAFKA_TOPIC=live-match-events
KAFKA_TOPIC_COMPETITIONS=football-competitions
KAFKA_TOPIC_LEADERBOARDS=football-leaderboards

# Football API (for streaming)
FOOTBALL_API_TOKEN=your_football_api_token
```

### **2. Install Dependencies**

```bash
# Install Python packages
pip install -r requirements.txt

# For streaming (optional)
pip install -r requirements-streaming.txt
```

### **3. Run Batch Pipeline**

```bash
# Execute full ETL pipeline
python run_pipeline.py
```

**Expected Output:**
```
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘              FOOTBALL DATA ETL PIPELINE                    â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

[1/7] ğŸ¥‰ Running Bronze Layer...
âœ… Bronze Layer Complete: 5,605,055 records in 52.3s

[2/7] ğŸ¥ˆ Running Silver Layer...
âœ… Silver Layer Complete: 5,535,614 records in 25.1s

[3/7] ğŸ¥‡ Running Gold Layer...
âœ… Gold Layer Complete: 402,992 records in 11.2s

[4/7] ğŸ“Š Loading Silver to PostgreSQL...
âœ… Silver loaded: 68,117 records in 13.4s

[5/7] ğŸ“Š Loading Gold to PostgreSQL...
âœ… Gold loaded: 402,992 records in 32.8s

[6/7] ğŸ” Creating Views...
âœ… Views created: 5 views in 0.2s

[7/7] âœ… Data Quality Validation...
âœ… All validation checks passed!

â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  PIPELINE COMPLETED SUCCESSFULLY IN 148.7 SECONDS          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
```

### **4. Run Streaming Pipeline (Optional)**

#### **Match Events Streaming** (NiFi pushes every 10s, Spark processes every 15s)
```bash
# Start match events streaming consumer
python src/streaming/spark_streaming_upsert.py

# Press Ctrl+C to stop gracefully
```

#### **Competitions & Leaderboards Streaming** (NiFi pushes every 60s, Spark processes every 60s)
```bash
# Start competitions streaming consumer
python src/streaming/spark_streaming_competitions.py

# Press Ctrl+C to stop gracefully
```

**Streaming Features:**
- âœ… **Global Checkpoint Config**: Auto-generates run IDs (UUIDs) per restart â†’ Batch ID resets to 0
- âœ… **Trigger Intervals**: Matches (15s), Competitions (60s) aligned with NiFi frequencies
- âœ… **JSONB Storage**: Flexible schema for live match events, competitions, leaderboards
- âœ… **UPSERT Logic**: Incremental updates to PostgreSQL (no duplicates)
- âœ… **Backpressure Handling**: `maxOffsetsPerTrigger` prevents Spark overload

**See [STREAMING_ARCHITECTURE.md](STREAMING_ARCHITECTURE.md) for complete streaming setup.**

---

## ğŸ“Š Data Overview

### **ğŸ“¦ Gold Analytics Tables (PostgreSQL)**

| Table | Records | Description |
|-------|---------|-------------|
| `player_analytics_360` | 92,671 | **360Â° player profiles**: Overall scores, peak values, career stats |
| `player_form_metrics` | 88,375 | **Performance metrics**: Goals, assists, per-90min rates |
| `market_value_trends` | 69,441 | **Market analysis**: Value trends, volatility, growth rates |
| `injury_risk_scores` | 34,561 | **Health analytics**: Injury history, risk scores, availability |
| `transfer_intelligence` | 117,944 | **Transfer insights**: Market intelligence, fee analysis |

### **ğŸ¥ˆ Silver Layer Tables (PostgreSQL)**

| Table | Records | Description |
|-------|---------|-------------|
| `player_profiles` | 92,671 | Cleaned player biographical data |
| `player_performances` | 4,965,850 | Match performance records |
| `player_market_value` | 248,175 | Historical market valuations |
| `transfer_history` | 117,944 | Transfer records |
| `player_injuries` | 34,561 | Injury history |
| `team_details` | 862 | Team information |
| `competition_details` | 59 | Competition metadata |
| ... | ... | 10 tables total |

### **ğŸ”´ Streaming Tables (PostgreSQL)**

| Table | Description | Schema |
|-------|-------------|--------|
| `football_matches` | Real-time live match updates | HYBRID (structured + JSONB) |

### **ğŸ” Views & Materialized Views**

| Type | Name | Description |
|------|------|-------------|
| View | `vw_top_players` | Top 100 players by overall score |
| View | `vw_high_value_low_risk_players` | High-value players with low injury risk |
| View | `vw_transfer_opportunities` | Best transfer targets |
| View | `vw_current_live_matches` | Currently live matches (streaming) |
| View | `vw_match_score_progression` | Score progression timeline (streaming) |
| Materialized | `mvw_position_statistics` | Aggregated stats by position |
| Materialized | `mvw_age_group_analysis` | Player statistics by age groups |
| Materialized | `mvw_league_statistics` | League-level aggregated statistics |

---

## ğŸ’» Sample Queries

### **Top 10 Players Overall**

```sql
SELECT 
    player_name,
    overall_player_score,
    peak_market_value,
    current_team,
    position
FROM analytics.player_analytics_360
ORDER BY overall_player_score DESC
LIMIT 10;
```

### **High-Value Low-Risk Players**

```sql
SELECT 
    player_name,
    current_market_value,
    injury_risk_score,
    total_injuries,
    position
FROM analytics.vw_high_value_low_risk_players
WHERE current_market_value > 10000000
ORDER BY overall_player_score DESC
LIMIT 20;
```

### **Market Value Trends**

```sql
SELECT 
    player_name,
    current_market_value,
    value_change_1y,
    value_change_3y,
    volatility_score
FROM analytics.market_value_trends
WHERE value_change_1y > 5000000
ORDER BY value_change_1y DESC;
```

### **Live Matches (Streaming)**

```sql
SELECT 
    competition_name,
    home_team_name || ' ' || ft_home_goals || '-' || ft_away_goals || ' ' || away_team_name AS match,
    status,
    minute
FROM streaming.football_matches
WHERE status = 'IN_PLAY'
ORDER BY updated_at DESC;
```

### **Position Statistics (Materialized View)**

```sql
SELECT 
    position,
    total_players,
    avg_market_value,
    avg_goals_per_90,
    avg_assists_per_90
FROM analytics.mvw_position_statistics
ORDER BY total_players DESC;
```

---

## ğŸ› ï¸ Tech Stack

### **Batch Processing**
- **Language**: Python 3.11+
- **Big Data**: Apache Spark 4.0.0 (PySpark)
- **Database**: PostgreSQL 14+
- **Storage**: Parquet (columnar format)
- **Libraries**: pandas, psycopg2, pyspark

### **Real-Time Streaming**
- **Producer**: Apache NiFi 1.25.0 (visual ETL)
- **Message Queue**: Confluent Cloud Kafka (managed)
- **Consumer**: Spark Structured Streaming
- **Format**: JSONB (flexible schema)
- **Authentication**: SASL/SSL

### **Visualization**
- **Dashboard**: Apache Superset 3.0.0
- **Charts**: Bar, Line, Pie, Heatmap, Table
- **Refresh**: Real-time (streaming) + Scheduled (batch)

### **Infrastructure**
- **Deployment**: Local installation (production-ready)
- **Version Control**: Git + GitHub
- **Security**: Environment variables, no hardcoded secrets
- **Data Volume**: 5.9M records, ~1.7 GB total

---

## ğŸ“ Project Structure

```
football_project/
â”œâ”€â”€ run_pipeline.py                 # ğŸš€ Main ETL orchestrator
â”œâ”€â”€ validate_data.py                # âœ… Data quality validation
â”œâ”€â”€ .env                            # ğŸ”’ Credentials (gitignored)
â”œâ”€â”€ .env.sample                     # ğŸ“‹ Template for .env
â”œâ”€â”€ requirements.txt                # ğŸ“¦ Python dependencies
â”œâ”€â”€ requirements-streaming.txt      # ğŸ“¦ Streaming dependencies
â”‚
â”œâ”€â”€ src/                            # ğŸ’» Source code
â”‚   â”œâ”€â”€ bronze_layer.py            # ğŸ¥‰ CSV â†’ Parquet ingestion
â”‚   â”œâ”€â”€ silver_layer.py            # ğŸ¥ˆ Data cleaning & standardization
â”‚   â”œâ”€â”€ gold_layer.py              # ğŸ¥‡ Analytics aggregation
â”‚   â”œâ”€â”€ data_quality_check.py      # âœ… Quality checks
â”‚   â””â”€â”€ streaming/                 # ğŸ”´ Streaming pipeline
â”‚       â”œâ”€â”€ spark_streaming_upsert.py         # Match events (15s trigger)
â”‚       â””â”€â”€ spark_streaming_competitions.py   # Competitions/Leaderboards (60s trigger)
â”‚
â”œâ”€â”€ schema/                         # ğŸ—„ï¸ Database schemas
â”‚   â”œâ”€â”€ analytics_schema.sql       # Gold layer DDL
â”‚   â”œâ”€â”€ streaming_schema.sql       # Streaming schema DDL
â”‚   â”œâ”€â”€ create_views_only.sql      # Views & materialized views
â”‚   â”œâ”€â”€ create_views.py            # View creation script
â”‚   â”œâ”€â”€ load_silver_to_postgres.py # Silver data loader
â”‚   â””â”€â”€ load_gold_to_postgres.py   # Gold data loader
â”‚
â”œâ”€â”€ football-datasets/              # ğŸ“Š Source data (5.6M records)
â”‚   â””â”€â”€ datalake/transfermarkt/    # Transfermarkt CSV files
â”‚
â”œâ”€â”€ jars/                           # â˜• Java dependencies
â”‚   â””â”€â”€ postgresql-42.7.1.jar      # PostgreSQL JDBC driver
â”‚
â””â”€â”€ docs/                           # ğŸ“š Documentation
    â”œâ”€â”€ PROJECT_OVERVIEW.md         # Complete system docs
    â”œâ”€â”€ STREAMING_ARCHITECTURE.md   # Streaming design
    â”œâ”€â”€ DATA_QUALITY.md             # Quality rules
    â””â”€â”€ CONFLUENT_CLOUD_SETUP.md    # Kafka setup guide
```

---

## ğŸ“š Documentation

### **ğŸ“– Getting Started**
- [README.md](README.md) - **This file** (overview + quick start)
- [PROJECT_OVERVIEW.md](PROJECT_OVERVIEW.md) - **Complete system documentation** (667 lines)
- [BAO_CAO_TONG_QUAN.md](BAO_CAO_TONG_QUAN.md) - **BÃ¡o cÃ¡o dá»± Ã¡n tiáº¿ng Viá»‡t** (Vietnamese report)

### **ğŸ“Š Database & Architecture Diagrams**
- [diagrams/DATABASE_SCHEMA.md](diagrams/DATABASE_SCHEMA.md) - **Complete ERD diagrams** (Silver, Gold, Streaming schemas)
- [diagrams/SYSTEM_ARCHITECTURE.md](diagrams/SYSTEM_ARCHITECTURE.md) - **System architecture diagrams** (Lambda, Medallion, ETL flows)

### **ğŸ”´ Streaming Setup**
- [STREAMING_ARCHITECTURE.md](STREAMING_ARCHITECTURE.md) - Architecture overview (428 lines)
- [CONFLUENT_CLOUD_SETUP.md](CONFLUENT_CLOUD_SETUP.md) - Kafka configuration

### **âœ… Data Quality**
- [DATA_QUALITY.md](DATA_QUALITY.md) - Quality rules and validation

---

## ğŸ”§ Configuration

### **Database Connection (Python)**

```python
import psycopg2
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Connect to PostgreSQL
conn = psycopg2.connect(
    host=os.getenv('POSTGRES_HOST', 'localhost'),
    port=os.getenv('POSTGRES_PORT', 5432),
    database=os.getenv('POSTGRES_DB', 'football_analytics'),
    user=os.getenv('POSTGRES_USER', 'postgres'),
    password=os.getenv('POSTGRES_PASSWORD')
)
```

### **Kafka Configuration (Streaming)**

```python
# Kafka connection settings
kafka_config = {
    'kafka.bootstrap.servers': os.getenv('KAFKA_BOOTSTRAP_SERVERS'),
    'kafka.security.protocol': 'SASL_SSL',
    'kafka.sasl.mechanism': 'PLAIN',
    'kafka.sasl.jaas.config': f'''org.apache.kafka.common.security.plain.PlainLoginModule required 
        username="{os.getenv('KAFKA_API_KEY')}" 
        password="{os.getenv('KAFKA_API_SECRET')}";''',
    'subscribe': os.getenv('KAFKA_TOPIC', 'live-match-events')
}
```

---

## ğŸ“ˆ Performance

### **Batch Pipeline Metrics**

| Layer | Duration | Records | Throughput |
|-------|----------|---------|------------|
| ğŸ¥‰ Bronze | 52s | 5,605,055 | 107,789 rec/s |
| ğŸ¥ˆ Silver | 25s | 5,535,614 | 221,424 rec/s |
| ğŸ¥‡ Gold | 11s | 402,992 | 36,635 rec/s |
| ğŸ“Š Load Silver | 13s | 68,117 | 5,240 rec/s |
| ğŸ“Š Load Gold | 33s | 402,992 | 12,212 rec/s |
| ğŸ” Views | 0.2s | 5 views | - |
| âœ… Validation | 10s | 8 checks | - |
| **â±ï¸ Total** | **~2.5 min** | **5.9M** | **39,333 rec/s avg** |

### **Streaming Pipeline Metrics**

| Metric | Value |
|--------|-------|
| Microbatch Interval | 30 seconds |
| Latency | < 1 minute (end-to-end) |
| Throughput | 17 matches/batch (average) |
| Write Mode | UPSERT (incremental) |
| Format | JSONB (flexible schema) |

### **Storage Metrics**

| Component | Size | Format |
|-----------|------|--------|
| Bronze Layer | 320 MB | Parquet |
| Silver Layer | 280 MB | Parquet |
| Gold Layer | 180 MB | Parquet |
| PostgreSQL DB | 935 MB | Relational |
| **Total** | **1.7 GB** | - |

---

## ğŸ¯ Use Cases

### **âš½ Player Scouting**
- Identify top performers by position, age, and market value
- Compare players across leagues and seasons
- Filter by specific metrics (goals/90, assists/90, etc.)

### **ğŸ’° Transfer Market Analysis**
- Find undervalued players with positive value trends
- Track market value volatility and growth rates
- Analyze transfer fees and patterns

### **ğŸ¥ Injury Risk Assessment**
- Evaluate player health and availability
- Calculate injury risk scores
- Monitor injury history and recovery times

### **ğŸ“Š Performance Analytics**
- Track player form and consistency
- Compare performance metrics across seasons
- Identify emerging talents

### **ğŸ”´ Real-Time Monitoring**
- Monitor live matches as they happen
- Track real-time statistics and events
- Build interactive dashboards with Apache Superset

---

## ğŸ“¸ Screenshots & Visualizations

### **Apache Superset Dashboards**

Our platform includes 5 comprehensive dashboards built with Apache Superset for data visualization and analysis:

#### **1. Player Scouting Dashboard**
![Player Scouting](dashboards/Player-Scouting.jpg)
*Identify top performers with detailed metrics across positions, leagues, and age groups. Features interactive filters for market value, overall score, and performance indicators.*

#### **2. Performance Analytics Dashboard**
![Performance Analytics](dashboards/Performance-Analytics.jpg)
*Track player form metrics, goals/assists trends, and statistical comparisons. Includes per-90 minute rates and consistency scores.*

#### **3. Transfer Market Intelligence Dashboard**
![Transfer Market Intelligence](dashboards/Transfer-Market-Intelligence.jpg)
*Analyze market value trends, identify undervalued players, and track transfer opportunities. Features value change percentages and volatility indicators.*

#### **4. Injury Risk Management Dashboard**
![Injury Risk Management](dashboards/Injury-Risk-Management.jpg)
*Monitor player health scores, injury history, and availability predictions. Helps with squad rotation and injury prevention strategies.*

#### **5. Football Leaderboards Dashboard**
![Football Leaderboards](dashboards/Football-Leaderboards.jpg)
*View competition standings, team rankings, and real-time tournament tables. Integrates streaming data for live updates.*

---

### **Apache NiFi Data Flow Processors**

Visual ETL flows designed in Apache NiFi for streaming data ingestion:

#### **Live Match Events Flow**
![Live Matches Flow](nifi/live-matches.png)
*InvokeHTTP processor fetches match data every 10 seconds from Football-Data.org API, routes by match status (IN_PLAY, FINISHED, SCHEDULED), and publishes to Kafka topic `live-match-events`.*

#### **Live Competitions Flow**
![Live Competitions Flow](nifi/live-competitions.png)
*Fetches competition metadata every 60 seconds, parses JSON structure, extracts competition IDs, and publishes to Kafka topic `football-competitions`.*

#### **Live Leaderboards Flow**
![Live Leaderboards Flow](nifi/live-leaderboards.png)
*Retrieves tournament standings every 60 seconds for each competition, parses team rankings and statistics, and publishes to Kafka topic `football-leaderboards`.*

#### **Competitions to Leaderboards Mapping**
![Competitions to Leaderboards](nifi/competitions-to-leaderboards.png)
*Intermediate processor that extracts competition IDs from the competitions flow and dynamically triggers leaderboard fetching for each tournament.*

---

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

---

## ğŸ“„ License

This project is for **educational and analytical purposes** only.

---

## ğŸ™ Acknowledgments

- **Data Source**: [Transfermarkt](https://www.transfermarkt.com/) football dataset
- **Architecture**: Medallion (Lakehouse) pattern
- **Inspiration**: Modern data engineering best practices

---

## ğŸ“ Contact

For questions or feedback, please open an issue on [GitHub](https://github.com/Flourish04/football_bigdata_analysis).

---

<div align="center">

**Made with âš½ + ğŸ + ğŸ˜ + ğŸ”¥**

[![Star this repo](https://img.shields.io/github/stars/Flourish04/football_bigdata_analysis?style=social)](https://github.com/Flourish04/football_bigdata_analysis)

</div>
