#!/usr/bin/env python3
"""
Load Events Layer to PostgreSQL
Loads match info, match statistics, and player event statistics
"""

import psycopg2
from pyspark.sql import SparkSession
import logging
from datetime import datetime

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# PostgreSQL connection details
PG_CONFIG = {
    'host': 'localhost',
    'port': 5432,
    'database': 'football_analytics',
    'user': 'postgres',
    'password': '9281746356'
}

JDBC_URL = f"jdbc:postgresql://{PG_CONFIG['host']}:{PG_CONFIG['port']}/{PG_CONFIG['database']}"
JDBC_PROPERTIES = {
    'user': PG_CONFIG['user'],
    'password': PG_CONFIG['password'],
    'driver': 'org.postgresql.Driver'
}


def create_events_schema():
    """Create events schema if not exists"""
    logger.info("üì¶ Creating events schema...")
    
    conn = psycopg2.connect(**PG_CONFIG)
    cursor = conn.cursor()
    
    try:
        cursor.execute("CREATE SCHEMA IF NOT EXISTS events;")
        conn.commit()
        logger.info("‚úÖ Events schema created/verified")
    except Exception as e:
        logger.error(f"‚ùå Failed to create schema: {e}")
        conn.rollback()
        raise
    finally:
        cursor.close()
        conn.close()


def execute_schema_sql():
    """Execute events_schema.sql to create tables"""
    logger.info("üìã Executing events_schema.sql...")
    
    conn = psycopg2.connect(**PG_CONFIG)
    cursor = conn.cursor()
    
    try:
        with open('schema/events_schema.sql', 'r') as f:
            schema_sql = f.read()
        
        cursor.execute(schema_sql)
        conn.commit()
        logger.info("‚úÖ Events schema SQL executed successfully")
    except Exception as e:
        logger.error(f"‚ùå Failed to execute schema SQL: {e}")
        conn.rollback()
        raise
    finally:
        cursor.close()
        conn.close()


def load_events_to_postgres():
    """Load events parquet data to PostgreSQL"""
    logger.info("="*80)
    logger.info("  LOADING EVENTS DATA TO POSTGRESQL")
    logger.info("="*80)
    
    start_time = datetime.now()
    
    # Initialize Spark
    spark = SparkSession.builder \
        .appName('LoadEventsToPostgres') \
        .config('spark.jars', 'jars/postgresql-42.7.1.jar') \
        .config('spark.driver.memory', '4g') \
        .config('spark.executor.memory', '4g') \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel('WARN')
    
    try:
        # Create schema and tables
        create_events_schema()
        execute_schema_sql()
        
        # 1. Load match_info
        logger.info("üìä Loading match_info...")
        match_info = spark.read.parquet("/tmp/football_datalake/events/match_info")
        
        match_info.write \
            .jdbc(url=JDBC_URL, table='events.match_info',
                  mode='overwrite', properties=JDBC_PROPERTIES)
        
        count = match_info.count()
        logger.info(f"‚úÖ Loaded {count:,} records to events.match_info")
        
        # 2. Load match_statistics
        logger.info("üìä Loading match_statistics...")
        match_stats = spark.read.parquet("/tmp/football_datalake/events/match_statistics")
        
        match_stats.write \
            .jdbc(url=JDBC_URL, table='events.match_statistics',
                  mode='overwrite', properties=JDBC_PROPERTIES)
        
        count = match_stats.count()
        logger.info(f"‚úÖ Loaded {count:,} records to events.match_statistics")
        
        # 3. Load player_event_statistics
        logger.info("üìä Loading player_event_statistics...")
        player_stats = spark.read.parquet("/tmp/football_datalake/events/player_event_statistics")
        
        # Drop id column (auto-generated by Postgres)
        player_stats = player_stats.drop("id") if "id" in player_stats.columns else player_stats
        
        player_stats.write \
            .jdbc(url=JDBC_URL, table='events.player_event_statistics',
                  mode='overwrite', properties=JDBC_PROPERTIES)
        
        count = player_stats.count()
        logger.info(f"‚úÖ Loaded {count:,} records to events.player_event_statistics")
        
        # Refresh materialized views
        logger.info("üîÑ Refreshing materialized views...")
        conn = psycopg2.connect(**PG_CONFIG)
        cursor = conn.cursor()
        cursor.execute("SELECT events.refresh_event_materialized_views();")
        conn.commit()
        cursor.close()
        conn.close()
        logger.info("‚úÖ Materialized views refreshed")
        
        duration = (datetime.now() - start_time).total_seconds()
        
        logger.info("="*80)
        logger.info("‚úÖ EVENTS DATA LOADED TO POSTGRESQL")
        logger.info(f"‚è±Ô∏è  Duration: {duration:.2f}s")
        logger.info("="*80)
        
    except Exception as e:
        logger.error(f"‚ùå Loading failed: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        spark.stop()


if __name__ == "__main__":
    load_events_to_postgres()
